\chapter{Experiment Design}\label{ch:experiment-design}

This chapter concisely describes how the system is configured, which routing algorithms are applied, what we measure, and how the experiments are executed. It is intentionally brief and code-aligned.

\section{Framework Configuration}\label{s:framework-config}
	extbf{Services.} Three backends operate in containers: MySQL (structured writes), Elasticsearch/Logstash/Kibana (search/analytics), and IPFS (tamper-evident archive). A Python orchestrator streams logs and invokes a pluggable router.

	extbf{Routing layer.} Routers live in \texttt{src/routers.py}: Static rules, Direct baselines (always route to one backend), Content-Based Routing (CBR), Tabular Q-Learning, and A2C. A compliance layer enforces sensitive-pattern overrides to IPFS (pre- and post-override destinations are recorded).

	extbf{Features.} Each decision uses a fixed-length observation built from: (i) a DistilBERT sentence embedding of the log message and (ii) lightweight system/context signals. RL policies may apply an optional scaler; Q-learning additionally uses PCA+quantile binning for a discrete state.

	extbf{Instrumentation.} Per-log latency is timed around the backend write; CPU package energy (RAPL) is read before/after when available. Measurements are streamed to per-router CSVs and summarized post-run.

\section{Routing Algorithms (Application)}\label{s:algorithms}
We evaluate five strategies; each returns a provisional destination in \{\texttt{MySQL}, \texttt{ELK}, \texttt{IPFS}\}.

\paragraph{Direct baselines.} Always choose a fixed backend $b$; establish simple bounds (e.g., all-MySQL for latency, all-IPFS for integrity).

\paragraph{Static heuristic.} A short rule list: security/kernel and critical levels $\to$ IPFS; errors/warnings or failure lexemes $\to$ ELK; otherwise MySQL. Used as baseline and safe fallback.

\paragraph{Content-Based Routing (CBR).} Learns a \emph{classifier attribute} from \{\texttt{Level}, \texttt{Component}, \texttt{LogSource}\} via variance-reduction scoring over bucketed cost samples. At decision time it queries the selected attribute’s bucket; if empty, use global backend means; otherwise fall back to the static policy. Excerpt (mirrors \texttt{CBRRouter.get\_route}):

\begin{lstlisting}[language=Python, caption={CBR fallback (excerpt from src/routers.py)}]
# If an attribute has been selected, try bucket-level expected costs
if self.classifier_attr:
    val = str(log_entry.get(self.classifier_attr, ""))
    bucket = self._bucket(val)
    best_backend, best_cost = None, float('inf')
    for backend in self.backend_order:
        el = self.expected_latency[self.classifier_attr].get((bucket, backend))
        if el is not None and el < best_cost:
            best_backend, best_cost = backend, el
    if best_backend is not None:
        return best_backend

# Otherwise fall back to global averages if we have any samples at all
if self.global_stats:
    best_backend, best_cost = None, float('inf')
    for backend, lst in self.global_stats.items():
        if not lst:
            continue
        mean_cost = sum(lst) / len(lst)
        if mean_cost < best_cost:
            best_backend, best_cost = backend, mean_cost
    if best_backend:
        return best_backend

# Final fallback: static heuristic
return self.static.get_route(log_entry)
\end{lstlisting}

\paragraph{Tabular Q-Learning.} Observations are standardized $\to$ PCA to $k$ dims $\to$ per-dim quantile bins, yielding a discrete state key. Action-values are updated with
\[
Q(s,a) \leftarrow Q(s,a) + \alpha\big(r + \gamma\max_{a'} Q(s',a') - Q(s,a)\big),
\]
with a teacher-guided exploration fraction that sometimes follows the static heuristic, and an adaptive-$\epsilon$ schedule that decays exploration only when returns plateau. Artifacts (scaler, PCA, binner, Q-table, metadata) are persisted and validated at load.

\paragraph{A2C.} An actor-critic network (shared MLP trunk) consumes the continuous observation; the actor outputs a categorical distribution over backends and the critic estimates state value. We optionally warm up an observation scaler, apply linear LR decay and entropy annealing, and keep the best-eval checkpoint for downstream evaluation.

\section{Evaluation Metrics}\label{s:metrics}
We report per-router summaries over the processed log stream:
\begin{itemize}
  \item Latency: mean/median, $p_{95}$, $p_{99}$ per backend and overall.
  \item Energy (if available): mean Joules per log and per backend.
  \item Combined cost: $\;C=\alpha\,\text{latency}+\beta\,\text{energy}$ (weights reported).
  \item Throughput: logs/sec over the evaluation window.
  \item Destination mix: fraction routed to MySQL/ELK/IPFS.
  \item Compliance: sensitive-coverage (to IPFS) and leakage (non-IPFS), plus override rate.
  \item Success rate: fraction of successful backend writes.
\end{itemize}

\section{Experimental Protocol}\label{s:protocol}
	extbf{Datasets.} We use two sources: (i) Loghub–Zenodo logs and (ii) a Synthetic Datacenter log set. Each is streamed deterministically (fixed seed) through the pipeline.

	extbf{Procedure.}
\begin{enumerate}
  \item Train RL (when applicable): Q-Learning on discretized states; A2C with on-policy rollouts. Persist artifacts and best checkpoints.
  \item Freeze RL for evaluation. CBR remains online-adaptive by design (bucket statistics keep updating via sampling).
  \item For each dataset and router (Direct baselines, Static, CBR, Q-Learning, A2C): route the full stream with the compliance layer enabled; record per-log CSV and summary CSV.
  \item Aggregate summaries to produce latency/energy distributions, destination mixes, success rates, and compliance metrics.
\end{enumerate}

	extbf{Outputs.} Results are written under \texttt{results/} as per-router per-dataset CSVs plus concise summary tables; figures are generated from these artifacts.

	extbf{Notes.} All services run in containers; seeds and configuration are captured alongside model artifacts for reproducibility. Evaluation uses deterministic policy inference (A2C greedy) unless stated otherwise.
\begin{itemize}
  \item Increase $E$ (less frequent evaluation) to reduce runtime overhead.
  \item Set $C > E$ if only a sparse subset of evaluation events require resilient persistence.
  \item Disable checkpoints ($C=0$) while retaining best model selection if disk space is constrained.
\end{itemize}

\paragraph{Metadata Persistence.}
Artifact metadata include:
\[
(C, \text{num\_checkpoints}, E, N_{\text{eval}}, \bar{R}_{\text{best}}, t_{\text{best}})
\]
mirroring \texttt{checkpoint\_interval}, \texttt{num\_checkpoints}, \texttt{eval\_interval}, \texttt{eval\_episodes}, \texttt{eval\_best\_mean\_reward}, and the (optional) step of the best model. This guarantees the reproducibility of the selection process.

\paragraph{Rationale.}
Separating concerns (periodic durability vs. performance-optimal snapshot) avoids conflating recency with quality and aligns with experimental reporting norms, where evaluation references the best-validated policy rather than the terminal weights.


\subsection{Compliance Enforcement Layer (Hard Override)}\label{s:compliance-layer}

Operational environments handling mixed-sensitivity telemetry often require \emph{non-negotiable} guarantees that security- or privacy-relevant records persist durably in an immutable audit trail. To meet this requirement, we introduce a thin, deterministic \emph{Compliance Enforcement Layer} that wraps any routing policy (static, CBR, Q-learning, A2C, or future methods) and enforces a hard override to the IPFS backend whenever a log entry matches a configurable set of sensitive textual patterns. This layer is deliberately orthogonal to learning; it neither alters rewards nor injects gradients; instead, it post-processes proposed actions at inference time, ensuring invariant coverage, irrespective of exploration or exploitation dynamics.

\subsubsection{Sensitive Pattern Detection}\label{s:compliance-pattern-detection}

Let $\mathcal{P} = \{p_1, \dots, p_m\}$ be a (small) set of user- and system-defined case-insensitive substring patterns (defaults include tokens such as \texttt{``sessionid''}, \texttt{``token''}, \texttt{``secret key''}, \texttt{``permission denied''}, \texttt{``login''}, and path fragments like \texttt{``/home/''}). For each incoming log with raw message text $T$, we classify it as sensitive if
\[
\exists p \in \mathcal{P}:\; p \text{ is a substring of } \text{lowercase}(T).
\]
This yields a binary predicate
\[
\texttt{is\_sensitive}(T) =
\begin{cases}
1\& \text{if match found},\\
0\& \text{otherwise}.
\end{cases}
\]
The pattern set can be extended at runtime: $\mathcal{P} \leftarrow \mathcal{P} \cup \mathcal{P}_{\text{user}}$, where $\mathcal{P}_{\text{user}}$ is supplied via CLI. Because detection is a simple substring search, its complexity is $O(|T| \cdot m)$ in the naive form; given small $m$, the overhead is negligible relative to embedding extraction and backend I/O. This choice favors explainability over heavier natural language processing (NLP) or regular expression (regex) pipelines.

\subsubsection{Override Semantics \& Metrics (Coverage, Leakage)}\label{s:compliance-metrics}

Let $a_{\text{raw}} \in \mathcal{A} = \{\text{MySQL}, \text{ELK}, \text{IPFS}\}$ be the backend proposed by the underlying router for log $i$, and $S_i = \texttt{is\_sensitive}(T_i)$. The enforced destination $a_i$ is
\[
a_i =
\begin{cases}
\text{IPFS},\& S_i = 1,\\
a_{\text{raw}},\& S_i = 0.
\end{cases}
\]
We record a modification flag
\[
\texttt{compliance\_forced}_i = \mathbb{1}[S_i = 1 \land a_{\text{raw}} \neq \text{IPFS}],
\]
and retains $a_{\text{raw}}$ for audit trails. Over $N$ logs:
\[
N_{\text{s}} = \sum_{i=1}^{N} S_i,\qquad
N_{\text{s}\rightarrow \text{IPFS}} = \sum_{i=1}^{N} \mathbb{1}[S_i = 1 \land a_i = \text{IPFS}],
\]
\[
\text{Coverage} \;=\; \frac{N_{\text{s}\rightarrow \text{IPFS}}}{N_{\text{s}}}, \qquad
\text{Leakage} \;=\; N_{\text{s}} - N_{\text{s}\rightarrow \text{IPFS}}.
\]
Leakage rate: $\text{LeakageRate} = \text{Leakage}/N_{\text{s}}$ (defined $0$ if $N_{\text{s}}=0$). Binary compliance score
\[
\text{ComplianceScore} =
\begin{cases}
1.0,\& \text{Leakage}=0,\\
0.0,\& \text{otherwise}.
\end{cases}
\]
Collateral immutable usage (cost/energy signal)
\[
\text{NonSensitiveIPFSFrac} = \frac{\sum_{i=1}^{N} \mathbb{1}[S_i = 0 \land a_i = \text{IPFS}]}{\max(1, N - N_{\text{s}})}.
\]
All are reported in cross-router summaries, enabling fair comparisons under enforced compliance.

\paragraph{Rationale.}
Coverage and leakage quantitatively separate surface governance effectiveness from routing intelligence. Any leakage ($>0$) is a configuration or pattern gap, not a learning failure.

\subsubsection{Separation from Policy Learning}\label{s:compliance-separation}

The layer defines an externally enforced transformed policy
\[
\pi'(a \mid s, T) =
\begin{cases}
\mathbb{1}[a = \text{IPFS}],\& \text{if } \texttt{is\_sensitive}(T)=1,\\
\pi(a \mid s),\& \text{otherwise,}
\end{cases}
\]
where $\pi$ is the underlying (possibly learned) router. During the training of adaptive methods:
\begin{enumerate}
  \item Rewards are computed using $a_{\text{raw}}$ (pre-override) to avoid biasing exploration toward IPFS merely due to compliance.
  \item The hard override applies only at evaluation / deployment emission time.
  \item Both $a_{\text{raw}}$ and final $a_i$ are logged, preserving the unperturbed policy trajectory for analysis.
\end{enumerate}
This strict separation prevents reward hacking and maintains a clear boundary between normative governance logic and performance and energy optimization. Future variants could add penalty shaping for near-misses; however, we intentionally excluded such coupling to maintain reproducibility and make the interpretation more straightforward.

\paragraph{Failure Modes \& Extensibility.}
False negatives degrade coverage; mitigation paths include expanding $\mathcal{P}$, Unicode normalization, or selective regexes. False positives inflated the NonSensitiveIPFSFrac (conservative bias). The mechanism is backend-agnostic: extending $\mathcal{A}$ leaves the enforcement semantics unchanged (all sensitive traffic still collapses to IPFS).

\paragraph{Summary.}
The Compliance Enforcement Layer guarantees immutability for sensitive logs, provides auditable governance metrics (coverage, leakage, collateral IPFS usage), and remains modular, enabling the independent evolution of both detection heuristics and adaptive routing policies.

\section{Energy \& Performance Measurement}
\label{s:energy-measurement}

\subsection{RAPL-Based Energy Acquisition}
\label{s:rapl-acquisition}
We obtained fine-grained CPU energy measurements using Intel's Running Average Power Limit (RAPL) interface, which exposes model-specific registers (MSRs) that aggregate package-level energy consumption. At decision time $t$ (one log routing + backend write), we read the package energy counter $E_{\text{pkg}}$ (joules) immediately before issuing the backend operation and again after completion, forming a differential as follows:
\[
\Delta E_t = E_{\text{pkg}}^{\text{after}} - E_{\text{pkg}}^{\text{before}}.
\]
RAPL counters monotonically increase with a hardware-defined wraparound; in our runs, the counter width (typically 32 or 48 bits) and total duration ensured no overflow. If a wrap is detected (negative raw delta), we ignore the sample for energy statistics and mark the log with an energy validity flag.

Access occurs via the Linux sysfs interface (e.g., \texttt{/sys/class/powercap/intel-rapl:0/energy\_uj}) or fallback MSR read, yielding microjoules, which we convert to joules by scaling $10^{-6}$. Each routing decision thus yields a $(\text{latency}_t, \Delta E_t)$ pair without additional synchronization: energy reads are low overhead ($<5\mu s$) relative to the backend I/O latency (ms scale). When RAPL is unavailable (non-Intel architectures or restricted environment), the system records $\Delta E_t = 0$ and flags energy as unsupported so that downstream combined cost metrics (Section~\ref{s:combined-cost}) can degrade gracefully.

We restricted energy attribution to the CPU package to maintain portability and reproducibility; DRAM or uncore domains were excluded to avoid partial availability across heterogeneous hosts and to prevent incomparable aggregates.

\subsection{Per-Log Attribution \& Units Normalization}
\label{s:per-log-energy-normalization}
Let $\text{latency}_t$ be the wall-clock end-to-end time (milliseconds) to ingest log $t$ through the chosen backend, and $\Delta E_t$ the CPU package energy (joules) defined above. Because latency and energy inhabit different numeric scales and exhibit different variance characteristics, we first ensured consistent units and then (optionally) formed normalized statistics.

We store raw:
\[
\text{latency}_t^{(\mathrm{ms})} \in \mathbb{R}_{\ge 0}, \qquad \Delta E_t^{(\mathrm{J})} \in \mathbb{R}_{\ge 0}.
\]

For reporting secondary metrics (e.g., Wh per 1000 logs or estimated $\text{CO}_2$), we applied deterministic conversions (not used inside learning rewards):
\[
\text{Wh}_{\text{batch}} = \frac{\sum_{t \in B} \Delta E_t}{3600}, \qquad
\text{CO}_2(B) = \text{Wh}_{\text{batch}} \cdot \kappa_{\text{grid}},
\]
where $\kappa_{\text{grid}}$ is the regional emission factor (kg $\text{CO}_2$/Wh). In the experiments, we fixed $\kappa_{\text{grid}}$ for relative comparability; absolute emission estimates are illustrative only.

To permit an energy-latency combined cost (Section~\ref{s:combined-cost}) that remains interpretable without per-run dynamic normalization, we deliberately avoid z-scoring in the cost function. Instead, we introduce a scalar coefficient $\lambda$ (units ms/J) that aligns the magnitude of the energy term with typical latency magnitudes (see next subsection). Outlier handling: If either measurement is missing (energy unsupported or timeout), we tag the sample and exclude it from the energy distribution plots while still retaining latency.

Finally, the per-log CSV outputs include:
\[
(\text{timestamp}_t, \text{backend}_t, \text{latency}_t^{(\mathrm{ms})}, \Delta E_t^{(\mathrm{J})}, \text{cost}_t, \text{flags}_t)
\]
where $\text{flags}_t$ encodes the energy validity to facilitate downstream aggregation filters.

\subsection{Combined Cost Formulation (Latency + $\lambda \cdot$ Energy)}
\label{s:combined-cost}
To expose an energy-performance trade-off to learning routers without obscuring interpretability, we define a linear scalarized per-log cost as follows:
\[
C_t = \text{latency}_t^{(\mathrm{ms})} + \lambda \,\Delta E_t^{(\mathrm{J})},
\]
where $\lambda$ (units ms/J) scales the joules into an equivalent latency penalty. Let $\bar{L}$ denote a representative latency scale (e.g., median of recent decisions) and $\bar{E}$ a representative energy scale (median joules). A principled initialization is as follows:
\[
\lambda^\star = \frac{\bar{L}}{\bar{E}},
\]
which balances the expected contributions of both terms at the median. In practice, we adopt a fixed $\lambda$ (default $1000$) chosen empirically so that the energy term neither dwarfs nor vanishes relative to typical millisecond latencies (package-joule magnitudes observed in our workload are on the order $10^{-3}$--$10^{-2}$ J, producing additive terms of $1$--$10$ ms).

This static scaling preserves (i) reproducibility across runs (no drifting normalization), (ii) additivity across logs (enabling simple summations for batch cost), and (iii) transparency (each joule interpreted as $\lambda$ ms budget impact).

Routers that optimize the expected return (RL) or select the minimum empirical cost (CBR) can internalize a unified objective:
\[
\min \ \mathbb{E}\big[C_t \mid \text{policy}\big] = \mathbb{E}[\text{latency}_t] + \lambda \,\mathbb{E}[\Delta E_t].
\]

Sensitivity: decreasing $\lambda$ biases toward latency-exclusive optimization; increasing $\lambda$ shifts the preference toward energy-efficient backends. Because the formulation is linear, the Pareto frontier exploration can be approximated by sweeping $\lambda$ and plotting $(\mathbb{E}[\text{latency}], \mathbb{E}[\Delta E])$ pairs.

If energy acquisition is unsupported (Section~\ref{s:rapl-acquisition}), we set $\Delta E_t = 0$ and the metric degenerates to pure latency.

\bigskip
\noindent\textbf{Section Summary.} We (i) capture low-overhead CPU package energy via RAPL, (ii) attribute per-log energy deltas paired with latency, and (iii) expose a tunable linear scalarization enabling routers to trade latency against energy explicitly while retaining the auditability of each raw dimension.






\section{Experimental Setup}\label{s:experimental-setup} 

This section details the empirical evaluation environment that underpins all the reported results. We first describe the datasets and stream construction strategies (Section~\ref{s:datasets-sampling}), and then enumerate the hardware and software baselines to support reproducibility (Section~\ref{s:hardware-software}). We formalize the experimental controls, that is, random seeds, episode definitions, and RL time-step budgets (Section~\ref{s:experimental-controls})before contrasting the offline training phases for reinforcement learning agents with the intrinsic online adaptation of CBR (Section~\ref{s:training-eval-protocol}). Finally, we catalog all emitted artifacts (logs, summaries, metadata, and checkpoints) to clarify provenance and enable independent replication (Section~\ref{s:outputs-artifacts}).

\subsection{Datasets \& Sampling Modes}
\label{s:datasets-sampling}

We evaluate two complementary log corpora chosen to exercise both the semantic diversity and scale sensitivity of adaptive routing:

\paragraph{Real-World (Loghub 2.0 Extract).} A curated subset (\(\sim 14{,}000\) entries) from the Loghub 2.0 collection covering heterogeneous system components (authentication, storage, scheduling) with a naturally occurring imbalance in severity levels and message templates. This dataset stresses \emph{semantic discrimination} (e.g., security-related tokens vs. routine status updates) while remaining tractable for rapid iteration and ablation studies.

\paragraph{Synthetic Datacenter Logs.} A procedurally generated corpus (\(\sim 200{,}000\) entries) emulating a multi-service data center. Parameterized generators produce controllable mixtures of (i) high-frequency low-severity operational noise, (ii) moderate-frequency performance anomalies, and (iii) low-frequency sensitive or security-relevant events. This scale emphasizes adaptation latency, bucket coverage (CBR), and exploration efficiency (RL).

\medskip
Each raw log record is transformed into: (i) discrete categorical attributes (e.g., \texttt{Level}, \texttt{Component}, \texttt{LogSource}), (ii) a DistilBERT embedding \(\mathbf{e} \in \mathbb{R}^{768}\) of the message text, and (iii) instantaneous system metrics (CPU utilization, queue depth, etc.) forming a concatenated observation vector of dimension 774 for learning-based routers (Sections~\ref{s:qlearning} and~\ref{s:a2c}).

\paragraph{Sampling Modes.} To mitigate bias from the sequential structure and enable controlled difficulty, we support three sampling regimes when presenting logs to routers:

\begin{itemize}
  \item \textbf{Head (Sequential Head Slice).} The first \(N\) lines of the file in the original order. Provides a reproducible fixed prefix that is useful for smoke tests and deterministic profiling.
  \item \textbf{Random.} Uniform sampling without replacement over the entire corpus size \(N\) and shuffling the presentation order. This reduces temporal locality and guards against order-sensitive overfitting (particularly for CBR bucket statistics).
  \item \textbf{Balanced.} Stratified sampling ensures approximately uniform counts across a chosen categorical attribute set (default: severity level) until exhaustion. When a stratum underflows before others, the residual capacity is filled by proportional allocation from the remaining strata. Balancing dampens majority class dominance and accelerates the acquisition of informative variance for both CBR attribute scoring and RL value estimation.
\end{itemize}

Let \(D = \{r_1,\dots,r_N\}\) be the dataset and \(\mathcal{S}\) the sampling operator producing an ordered stream \(S = (r_{i_1}, r_{i_2}, \dots, r_{i_M})\). For the balanced mode over strata \(\mathcal{C} = \{c_1,\dots,c_K\}\) with counts \(n_k\), we target \(q_k = \left\lceil \frac{M}{K} \right\rceil\) per class, falling back to \(\min(q_k, n_k)\) with iterative reallocation of surplus. All modes record the effective permutation (or allocation table) in the run metadata for auditability and reproducibility.

\paragraph{Rationale.} Presenting multiple sampling modes enables stress-testing robustness: policies should (i) retain gains under distribution reshuffling (Random), (ii) demonstrate rapid warm-up under scarce but balanced high-signal events (Balanced), and (iii) degrade gracefully when facing realistically skewed sequential prefixes (Heads). We report the metrics per mode where space permits; primary comparisons use the mode indicated in each figure caption for clarity.

\begin{table}[htbp]
\centering
\footnotesize
\caption{Log dataset schema}
\label{tab:dataset-schema}
\begin{tabularx}{\linewidth}{@{}l l X@{}}
\toprule
\textbf{Field} & \textbf{Type} & \textbf{Description} \\
\midrule
LineId & Integer & Sequential line/index of the log entry in the source file. \\
Time & String & Timestamp as found in the log (\texttt{Sun Dec 04 04:47:44 2005} or \texttt{18:06:20,076}). \\
EventId & String & Template identifier assigned after parsing (\texttt{E2}, \texttt{E91}). \\
Level & String & Log level/severity (\texttt{notice}, \texttt{INFO}, \texttt{ERROR}, \texttt{WARN}). \\
EventTemplate & String & Normalized message pattern with placeholders \texttt{<*>}. \\
Content & String & Original/raw log message text. \\
Component & String & Emitting component/module/class, when available. \\
Date & Date & Calendar date when stored separately from \texttt{time}. \\
Node & String & Hostname/node that emitted the log (if present). \\
LogSource & String & System or dataset source. \\
\bottomrule
\end{tabularx}
\end{table}




\subsection{Hardware \& Software Environment}
\label{s:hardware-software}

\paragraph{Host Hardware.} The experiments were run on a single Linux host with the following specifications:
\begin{itemize}
  \item \textbf{CPU:} Intel (x86\_64) processor supporting RAPL package energy counters.
  \item \textbf{Cores/Threads:} (Logical core count documented in run metadata).
  \item \textbf{Memory:} Sufficient system RAM to hold both datasets in page cache (actual GB value recorded in metadata).
  \item \textbf{Storage:} SSD-backed filesystem ensuring low jitter for MySQL and Elasticsearch writes.
\end{itemize}
We isolated the competing background load (no extraneous CPU-intensive processes) and fixed the CPU frequency scaling to the default governor (not manually pinned) to reflect the typical deployment conditions. Thermal throttling was not observed (checked via the absence of anomalous latency spikes correlated with dmesg thermal messages).

\paragraph{Virtualized Services (Docker).} The three storage backends (MySQL, Elasticsearch/Kibana (ELK), and IPFS node/daemon) are provisioned via a single \texttt{docker-compose. yml} file, ensuring consistent container versions across runs. Container resource limits (CPU shares, memory) were left at defaults to avoid artificial contention skew unless otherwise noted. This mirrors a pragmatic lightweight deployment rather than a tightly resource-sliced cluster.

\paragraph{Software Stack.}
\begin{itemize}
  \item \textbf{Operating System:} Linux kernel (version captured in experiment metadata).
  \item \textbf{Python:} Version $\geq$ 3.9 (exact minor version logged).
  \item \textbf{Key Libraries:} \texttt{transformers} (DistilBERT embeddings), \texttt{stable-baselines3} (A2C), \texttt{scikit-learn} (PCA, discretization), \texttt{pandas}/\texttt{numpy} (data handling).
  \item \textbf{RL Reproducibility:} Global RNG seeds (Python, NumPy, PyTorch) set when provided; PyTorch deterministic flags left at defaults to avoid performance penalty (documented in metadata).
\end{itemize}

\paragraph{Energy Measurement Subsystem.} RAPL readings extracted from the sysfs powercap interface (\texttt{intel-rapl:0}) for every routing decision. A lightweight abstraction caches the file descriptors to minimize the overhead. If RAPL is unavailable, the system sets an \texttt{energy\_supported = false} flag; downstream combined cost computations automatically revert to latency-only.

\paragraph{Backend Configuration.}
\begin{itemize}
  \item \textbf{MySQL:} Default transactional table engine (InnoDB). Autocommit retained. No query caching modifications were made.
  \item \textbf{Elasticsearch:} Default index settings (single node). Explicit refresh is not forced per insert; we rely on eventual indexing to reflect the typical operational write path latency. Latency measurement stops after client acknowledgment and not after searchability.
  \item \textbf{IPFS:} Local daemon with default pin and garbage collection settings; add operations measured until hash receipt.
\end{itemize}

\paragraph{Isolation \& Fairness.} All routers share an identical log presentation order within a run configuration (same sampled stream), enforced by persisting the realized permutation/allocation. RL agents are evaluated \emph{frozen} (no parameter updates) in test mode, ensuring that online adaptation advantages accrue solely to CBR (Section~\ref{s:training-eval-protocol}). Between router evaluations, the container state (indexes, table growth) is intentionally \emph{not} reset, reflecting cumulative storage growth conditions; this favors realism over a sterile cold-start per method.

\paragraph{Monitoring.} Latency is measured wall-clock around the exact backend write call. Energy differential windows tightly bracket only the routing decisions and backend I/O code paths. Outlier detection (e.g., due to transient Docker maintenance tasks) is limited to post-hoc reporting—no runtime filtering beyond counter wrap detection—to preserve the transparency.



\subsection{Experimental Controls (Seeds, Episode Lengths, Timesteps)}
\label{s:experimental-controls}

A robust comparison of heterogeneous routing strategies requires tightly specified control levers governing the stochasticity, training horizon, and evaluation scope. We standardized these along four axes: random seed handling, episode semantics for tabular Q-learning, time-step budgeting for A2C, and stream reuse across routers.

\paragraph{Random Seeds.} When a seed $s$ is provided (CLI flag \texttt{--seed}), we deterministically set Python's \texttt{random}, NumPy, and PyTorch RNGs to $s$, and record $s$ in all metadata artifacts (Sections~\ref{s:qlearning} and~\ref{s:a2c}). The absence of a seed yields non-deterministic (host RNG–driven) runs, which are explicitly tagged \texttt{seed: null} to prevent false assumptions of repeatability. The Docker container instantiation order is not separately fixed; however, its effect on our measurements is negligible because the backend warm caches are intentionally permitted (Section~\ref{s:hardware-software}).

\paragraph{Episode Definition (Q-learning).} Tabular Q-learning training proceeds over $E$ episodes (user flag \texttt{--q\_episodes}). An episode presents a fixed-length slice of the sampled log stream until either (i) a maximum per-episode decision count $L_{\max}$ is reached or (ii) the stream is exhausted. In practice, we set $L_{\max}$ equal to the stream length divided by $E$ (integer floor) to distribute coverage. Terminal states are only episodic boundaries; no environmental reset semantics alter the feature extraction pipeline. The reward is accumulated per episode and used for adaptive epsilon plateau detection (Section~\ref{s:qlearning-adaptive-eps}).

\paragraph{Timestep Budget (A2C).} Advantage Actor–Critic optimization is parameterized by the total training time steps $T$ (flag \texttt{--timesteps}) and rollout horizon $n$ steps per update (flag \texttt{--n\_steps}). With $n_{\text{env}}$ parallel environments (default 1), the number of gradient updates is:
\[
U = \left\lfloor \frac{T}{n \cdot n_{\text{env}}} \right\rfloor.
\]
The learning rate linear decay (if enabled) interpolates from $lr_0$ to $0$ across $T$ steps; entropy coefficient annealing (if enabled) interpolates over a configured subrange $T_{\text{ent}} \le T$. Evaluation (if \texttt{--eval\_interval} $>0$) pauses training every $I$ time steps to run $K$ evaluation episodes using the frozen policy snapshot, optionally updating the persistent best model.

\paragraph{Stream Reuse Across Routers.} For a given run configuration (dataset + sampling mode + seed), we materialized the ordered log stream once and reused it across all evaluated routers (CBR, Q-learning, A2C, and direct baselines). This isolates policy quality differences from presentation order artifacts. CBR uniquely updates statistics online during evaluation, whereas RL agents (both tabular and deep) are strictly inference-only in the evaluation mode.

\paragraph{Warm-Up Phases.} (i) CBR employs a sample-count warm-up before the first attribute selection (flag \texttt{--cbr\_warm\_samples}); (ii) A2C optionally performs a scaler warm-up (\texttt{--scaler\_warmup}) collecting raw observations without parameter updates; (iii) Q-learning initializes unseen state-action values with either zeros or a teacher-derived prior (Section~\ref{s:qlearning-guided}). These warm-up mechanics are orthogonal and are logged distinctly in the metadata to prevent the misinterpretation of the initial low-throughput intervals.

\paragraph{Epsilon / Exploration Schedules.} The base geometric decay of Q-learning (if enabled) and plateau-triggered adaptive drops are fully parameterized via CLI flags and written to metadata version 3 (Section~\ref{s:qlearning-adaptive-eps}). A2C exploration arises implicitly from policy stochasticity; optional entropy annealing (Section~\ref{s:a2c-schedules}) modulates the variance but does not require per-episode manual resets.

\paragraph{Reproducibility Assurance.} Each aggregated summary CSV embeds (i) dataset fingerprint (SHA-1 over raw CSV bytes), (ii) effective sampling mode, (iii) seed (or null), and (iv) RL training reuse indicators (boolean). This minimal provenance bundle suffices to reconstruct the exact evaluation stream and align it with the stored model artifacts.

\subsection{Routing Methods Under Test}
We evaluated six routing strategies to provide a comprehensive comparison.

\begin{itemize}
    \item \textbf{Three Fixed Baselines:} Each routes all logs to a single backend (MySQL-only, Elasticsearch-only, IPFS-only). These serve as simple performance baselines and sanity checks for the measurement pipelines.
    
    \item \textbf{Static Router:} Uses predefined, heuristic rules (route kernel/SSH traffic to IPFS, error-heavy logs to Elasticsearch, routine messages to MySQL).
    
    \item \textbf{Q-learning Router:} Learns a policy via tabular Q-learning on a discretized state space, using PCA and quantile binning. Employs $\varepsilon$-greedy exploration with a bias towards the static policy.
    
    \item \textbf{A2C Router:} Learns a policy using the A2C algorithm from the Stable-Baselines3 library, training on the same reward signal.
\end{itemize}

\subsection{Trainings \& Evaluation Protocol (Offline RL vs Online CBR)}
The evaluation was conducted in two distinct phases, as shown in Figure~\ref{fig:experiment-pipeline}.

\textbf{Phase 1: Training.} The learning-based routers (Q-learning and A2C) were trained in the environment.

\begin{itemize}
    \item \textbf{Q-learning:} The agent collected a warm-up buffer (5,000 observations) to fit its PCA model and quantile binners. It was then trained for 2,000 episodes using $\varepsilon$-greedy exploration.
    
    \item \textbf{A2C:} The agent was trained for a fixed number of time steps using on-policy rollouts.
\end{itemize}

Artifacts (Q-tables, PCA models, and Stable-Baselines3 checkpoints) were saved for Phase 2.

\textbf{Phase 2: Evaluation.} All six routing methods were evaluated using the same sampled log stream. The learned models were loaded and executed deterministically (without any exploration). For each log write, we recorded the routing latency, backend latency, success status, and CPU energy consumption.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.99\linewidth]{vu-cs-standard-thesis/0_frontmatter/figures/eval2.png}
\caption{The two-phase experimental pipeline for training and evaluating routing policies.}
\label{fig:experiment-pipeline}
\end{figure}



\subsection{Outputs \& Logged Artifacts}
\label{s:outputs-artifacts}

To support transparency, post hoc analysis, and reproducibility, the system emits a layered set of artifacts spanning per-log traces, aggregated summaries, model checkpoints, and diagnostic metadata. Table~\ref{tab:artifact-summary} (optional if space permits) summarizes these; we detail the semantics below.

\paragraph{Per-Log Traces.} For each router $r$ we write
\[
\texttt{results/}<r>_<\text{dataset}>.\texttt{csv}
\]
containing one row per processed log with the following columns:
\[
(\text{idx}, \text{timestamp}, \text{backend}, \text{raw\_destination}, \text{latency\_ms}, \Delta E_{\text{J}}, \text{cost}, \text{compliance\_forced}, \text{flags})
\]
where (i) \texttt{raw\_destination} is the router's pre-compliance choice (identical to \texttt{backend} if not overridden), (ii) \texttt{flags} encodes energy validity/fallback events, and (iii) \texttt{cost} is latency or combined cost depending on the run configuration. Traces enable a fine-grained latency distribution and per-attribute diagnostics.

\paragraph{Per-Router Summaries.} For each router we emit
\[
\texttt{results/summary\_}<r>_<\text{dataset}>.\texttt{csv}
\]
capturing scalar metrics: mean / median / p95 latency, success rate, destination mix proportions, average energy per log (J), combined cost components, and compliance coverage metrics (if enabled). These isolate performance characteristics without cross-router aggregation noise.

\paragraph{Combined Summary.} A single
\[
\texttt{results/combined\_summary\_}<\text{dataset}>.\texttt{csv}
\]
joins all router summaries side-by-side, adding relative improvement deltas over a configurable baseline (default: best-direct backend). Optional Markdown rendering (\texttt{--write\_markdown}) produces a companion \texttt{.md} table for direct inclusion in reports.

\paragraph{Experiment Metadata.} File
\[
\texttt{results/experiment\_metadata\_}<\text{dataset}>.\texttt{json}
\]
contains the dataset fingerprint (SHA-1), sampling mode, realized stream length, seed (or null), wall-clock durations per phase (training vs. evaluation), RL training reuse booleans, git commit hash, environment identifiers (Python version, library versions), and feature vector dimension. This JSON is the canonical provenance anchor.

\paragraph{RL Artifacts.}
\begin{itemize}
  \item \textbf{Q-learning:} \_q\_table.pkl, \_scaler.pkl, \_pca.pkl, \_binner.pkl, \_metadata.json (versioned schema with embedding dimension, exploration config, adaptive epsilon events).
  \item \textbf{A2C:} base.zip (final policy), \_best.zip (optional best eval model), \_metadata.json (version 1 fields: hyperparameters, schedules, reward stats), \_scaler.pkl (if warm-up used), intermediate checkpoints (if \texttt{--checkpoint\_interval} specified).
\end{itemize}
All model metadata embed a \texttt{version} integer; the loading code enforces forward compatibility by validating the required fields and dimensions (falling back when mismatched).

\paragraph{CBR Diagnostics.} When enabled:
\[
\texttt{cbr\_diag.json} \text{ (overwrite / append / timestamp modes)}
\]
periodically records the attribute score rankings, selected classifier, per-bucket sample counts, and backend cost aggregates. This supports interpretability (e.g., verifying when a high-gain attribute is adopted).

\paragraph{Compliance Metrics Augmentation.} If compliance is active, per-router summary and combined summary rows include: \texttt{sensitive\_total}, \texttt{sensitive\_coverage}, \texttt{leakage\_rate}, \texttt{non\_sensitive\_ipfs\_fraction}, \texttt{compliance\_score}. The effective sensitive pattern list was duplicated into the experiment metadata to avoid ambiguity due to evolving defaults.

\paragraph{Reward / Exploration Logs.}
\begin{itemize}
  \item Q-learning: optional reward history CSV and adaptive event NDJSON (Section~\ref{s:qlearning-adaptive-eps}).
  \item A2C: optional \texttt{a2c\_rewards.csv} plus TensorBoard event files (policy loss, value loss, entropy, learning rate) under \texttt{runs/}.
\end{itemize}

\paragraph{Scalar Energy\& Emissions Aggregates.} Batch-level energy (Wh) and illustrative CO$_2$ estimates were recomputed from per-log \(\Delta E_t\) (Section~\ref{s:per-log-energy-normalization}) during summary generation; we do not persist a separate emissions file to avoid redundancy.

\paragraph{Failure / Fallback Indicators.} Any artifact incompatibility (dimension or version mismatch) triggers the following:
\begin{enumerate}
  \item A warning entry in experiment metadata.
  \item Router-level flags in combined summary (\texttt{artifact\_valid=false}, \texttt{fallback=static}).
  \item Continuation of evaluation with static decisions for comparability.
\end{enumerate}

\paragraph{Retention Strategy.} Large intermediate checkpoint sets (A2C) are optional; users can disable them by omitting \texttt{--checkpoint\_interval}. The CBR diagnostic frequency should be tuned to balance interpretability vs. I/O overhead (default disabled). All file naming is deterministic, given artifact prefixes to simplify cleanup and version control ignoring.

Optional artifact summary table (include if space)
\begin{table}[h]
\centering
\caption{Artifact categories and purposes.}
\label{tab:artifact-summary}
\begin{tabular}{ll}
\toprule
Artifact\& Purpose \\
\midrule
Per-log CSV\& Raw decision trace for distributional / attribution analysis \\
Summary CSV\& Router-specific scalar metrics \\
Combined summary\& Cross-router comparative metrics + deltas \\
Experiment metadata JSON\& Provenance, configuration, environment \\
Q-learning metadata\& State abstraction + exploration provenance \\
A2C metadata\& Hyperparameters, schedules, reward statistics \\
CBR diagnostics JSON\& Online attribute gain evolution (optional) \\
Reward history / events\& Learning dynamics visualization \\
Checkpoints\& Temporal snapshots (A2C) for robustness / rollback \\
\bottomrule
\end{tabular}
 \end{table}

\paragraph{Reproducibility Path.} Given (i) dataset CSV, (ii) experiment metadata JSON, and (iii) RL artifact directories, a third party can reconstruct the evaluation stream order, reload policies, and regenerate combined summaries deterministically (modulo nondeterministic low-level kernel scheduling noise within single-digit microseconds, immaterial to the reported metrics).

This structured output ensures a complete provenance trail, allowing for the 
reproduction of all the figures and results. The complete flow of data from the 
system to these output artifacts is shown in Figure~\ref{fig:export}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.99\linewidth]{vu-cs-standard-thesis/0_frontmatter/figures/data_export_flow.png}
\caption{Data export and artifact generation flow.}
\label{fig:export}
\end{figure}



\section{Orchestration \& Automation}
\label{s:orchestration}

\subsection{Unified Pipeline (Training Detection \& Reuse)}
\label{s:orchestration-unified-pipeline}
To ensure methodological fairness and eliminate configuration drift between router evaluations, we implemented a unified orchestration layer (implemented in \texttt{src/experiment. py}) that encapsulates: (i) artifact presence  and compatibility checks, (ii) conditional (re-)training of learning-based routers (Q-Learning and A2C), and (iii) a single-pass evaluation stream over the selected dataset for all routers. Let $\mathcal{R}_{\text{learn}} = \{\text{Q}, \text{A2C}\}$ denote routers requiring offline training and $\mathcal{R}_{\text{direct}} = \{\text{CBR}, \text{direct\_mysql}, \text{direct\_elk}, \text{direct\_ipfs}\}$ those that are training-free (CBR adapts online but has no pre-training phase). Before evaluation, the orchestrator computes for each $r \in \mathcal{R}_{\text{learn}}$:
\[
\text{NeedsRetrain}(r) := \neg \text{AllArtifactsPresent}(r) \;\lor\; \neg \text{MetadataCompatible}(r) \;\lor\; \text{ForceRetrainFlag},
\]
where \texttt{MetadataCompatible} validates the schema version, embedding dimensionality, PCA/binning configuration (Q-Learning), scaler presence expectations, and hyperparameter signatures. Only routers with $\text{NeedsRetrain}(r)=\text{true}$ incur training costs; the others reuse existing artifacts, guaranteeing consistent baselines across repeated experimental runs while avoiding unnecessary recomputation (time  and energy savings).

A single canonical sampling configuration (dataset path + sampling mode + random seed) materializes an ordered sequence of logs that are streamed identically to every evaluated router to ensure strict comparability (no reshuffling-induced variance). For learning-based routers, parameters affecting training (episodes, time steps, entropy/epsilon schedules, and scaler warm-up length) are captured in \emph{training metadata} and duplicated into the final experiment metadata bundle to enable full-provenance reconstruction.

\paragraph{Execution Flow.}
\begin{enumerate}
    \item Parse CLI arguments; compute dataset fingerprint (hash of filename, size, plus hash of first/last $k$ lines).
    \item For each $r \in \mathcal{R}_{\text{learn}}$: evaluate $\text{NeedsRetrain}(r)$; if true, launch training and persist artifacts $\mathcal{A}_r$ (model weights, transformers, metadata).
    \item Instantiate evaluation objects: freeze parameters for $\mathcal{R}_{\text{learn}}$; initialize CBR cold (empty statistics).
    \item Stream log entries $(\ell_1,\ldots,\ell_N)$ once; for each router $r$ apply its routing decision (post compliance override if enabled), measure latency and energy, append a per-log trace row.
    \item After all routers complete, aggregate metrics (Section~\ref{s:orchestration-aggregation}), write cross-router summary and experiment metadata JSON (including which trainings were reused vs.\ performed fresh).
\end{enumerate}

\paragraph{Determinism Guards.} Global seeds (Python, NumPy, PyTorch) plus a fixed-order iterator over the log stream enforce reproducibility given identical artifact versions and host environment (cf.~Experimental Setup Section~\ref{s:experimental-setup}). A failed compatibility check (e.g., changed embedding length) triggers a safe fallback: the obsolete artifact is ignored, and retraining proceeds, preventing silent misalignment.

\subsection{Aggregation \& Metric Computation}
\label{s:orchestration-aggregation}
Post-evaluation aggregation transforms per-log traces into per-router summaries and a cross-router comparison table. Let
\[
D_r = \{ (t_i^{(r)}, e_i^{(r)}, s_i^{(r)}, \mathbf{1}_{\text{success},i}^{(r)}, \mathbf{1}_{\text{sensitive},i}, d_i^{(r)}) \}_{i=1}^{N}
\]
denote, for router $r$, total latency (ms), energy (J), raw backend status, success indicator, sensitivity indicator (compliance classification), and final destination category, respectively. Metrics:
\[
\text{MeanLatency}(r)=\frac{1}{N}\sum_{i=1}^{N} t_i^{(r)}, \qquad
\text{MedianLatency}(r)=\text{P}_{50}(\{t_i^{(r)}\}), \qquad
\text{P95Latency}(r)=\text{P}_{95}(\{t_i^{(r)}\}),
\]
\[
\text{SuccessRate}(r)=\frac{1}{N}\sum_{i=1}^N \mathbf{1}_{\text{success},i}^{(r)}, \qquad
\text{EnergyMean}(r)=\frac{1}{N}\sum_{i=1}^N e_i^{(r)}.
\]
Destination mix proportions:
\[
\pi_{b}(r)=\frac{1}{N}\sum_{i=1}^N \mathbf{1}[d_i^{(r)}=b], \quad b \in \{\text{mysql},\text{elk},\text{ipfs}\}.
\]
Compliance-aware metrics (evaluated per router after overrides)
\[
\text{SensitiveCoverage}(r)=\frac{\sum_{i=1}^N \mathbf{1}_{\text{sensitive},i}\cdot \mathbf{1}[d_i^{(r)}=\text{ipfs}]}{\sum_{i=1}^N \mathbf{1}_{\text{sensitive},i}}, \quad
\text{Leakage}(r) = \sum_{i=1}^N \mathbf{1}_{\text{sensitive},i}\cdot \mathbf{1}[d_i^{(r)}\neq \text{ipfs}],
\]
with $\text{ComplianceScore}(r) = \mathbf{1}[\text{Leakage}(r)=0]$.

\paragraph{Streaming Computation.} Latency percentiles (median, p95) are derived via exact sort (feasible at our scale) or optionally a Greenwald--Khanna sketch (not enabled by default). Aggregation algorithm:
\begin{enumerate}
    \item Initialize accumulators (sums, destination counts, sensitive counters).
    \item Single pass over each per-log CSV; update sums and counts; append latency to a list.
    \item Compute percentiles; compose per-router summary row; write summary CSV.
    \item After all routers, emit combined summary table (CSV; optional Markdown).
\end{enumerate}

\paragraph{Energy Normalization.} When the RAPL energy is unsupported (all zeros), the summaries retain energy columns plus a metadata flag marking the acquisition status to prevent misinterpretation as true zero consumption.

\paragraph{Robustness.} Partial evaluation (router crash) yields a \texttt{status=incomplete} marker; such routers are excluded from cross-router comparative statistics while preserving the raw traces for diagnosis.

\subsection{Compliance vs Baseline Run Pairing}
\label{s:orchestration-compliance-pairing}
To isolate the marginal effect of hard compliance enforcement on routing efficiency, we adopted a paired-run design: for the same dataset sampling configuration and identical pre-trained artifacts, we executed two orchestrated evaluations: (a) baseline (compliance disabled) and (b) compliance-enabled (pattern override active). Let a paired set be indexed by $k$ with a shared log sequence $\{\ell_i^{(k)}\}_{i=1}^{N_k}$. For any router $r$, define the paired deltas as
\begin{align}
\Delta \text{MeanLatency}_r^{(k)} 
  &= \text{MeanLatency}_r^{(k,\text{comp})} - \text{MeanLatency}_r^{(k,\text{base})}, \\
\Delta \text{EnergyMean}_r^{(k)} 
  &= \text{EnergyMean}_r^{(k,\text{comp})} - \text{EnergyMean}_r^{(k,\text{base})}.
\end{align}

Destination redistribution toward IPFS:
\[
\Delta \pi_{\text{ipfs},r}^{(k)} = \pi_{\text{ipfs}}^{(k,\text{comp})}(r) - \pi_{\text{ipfs}}^{(k,\text{base})}(r).
\]
Because log ordering and router stochasticity sources (RL policies frozen at evaluation) are held constant, the variance of these deltas reflects only compliance-induced routing path changes. For $K$ paired datasets (or repeated seeds), we report the mean and optional $95\%$ confidence intervals:
\[
\overline{\Delta \text{MeanLatency}}_r = \frac{1}{K} \sum_{k=1}^K \Delta \text{MeanLatency}_r^{(k)}, \qquad
\text{CI}_{95} = t_{0.975, K-1}\cdot \frac{\sigma_{\Delta}}{\sqrt{K}},
\]
where $\sigma_{\Delta}$ is the sample standard deviation of paired differences.

\paragraph{Leakage Sanity.} Any non-zero leakage in a compliance-enabled run aborts the aggregation of deltas (guard-rail for regression in pattern detection). A compliance integrity flag was recorded in the experimental metadata to confirm leak-free enforcement.

\paragraph{Interpretation.} A small positive $\Delta \text{MeanLatency}$ with zero leakage and modest $\Delta \text{EnergyMean}$ indicates limited performance overhead, and a large $\Delta \pi_{\text{ipfs},r}$ quantifies the archival shift. This frames the governance versus efficiency trade-off.

\paragraph{Reconstruction.} Each paired evaluation stores a harmonized metadata bundle containing the commit hash, artifact hashes, pattern list, dataset fingerprint, compliance flag, and orchestrator parameter snapshot; thus, any reported delta metric is reproducible by re-running the orchestrator with that configuration.

\paragraph{Summary.} The orchestrator centralizes (i) the artifact lifecycle, (ii) deterministic evaluation, (iii) robust aggregation, and (iv) paired compliance impact analysis, providing a reproducible methodological spine for all empirical claims in this study.

\section{Implementation Details}\label{s:implementation-details}

\subsection{Key Code Snippets}
This subsection highlights the concrete implementation patterns that instantiate the methodological design described earlier (routing abstraction, state transformation, adaptive learning, compliance-ready aggregation, and energy instrumentation). We excerpt only the critical fragments; the full source resides under \texttt{src/}.

\paragraph{Router Abstraction.}
All routing strategies conform to a minimal interface: a pure decision method plus an optional feedback hook for online learners (CBR).
\begin{lstlisting}[language=Python,caption={Router interface and Static baseline (excerpt)},label={lst:router-base}]
class BaseRouter(ABC):
    @abstractmethod
    def get_route(self, log_entry: dict) -> str:
        """Return one of: 'mysql', 'elk', 'ipfs'."""
        raise NotImplementedError

    def observe(self, *, log_entry: dict, destination: str, success: bool,
                routing_latency_ms: float, backend_latency_ms: float,
                energy_cpu_pkg_j: float | None = None):
        return None

class StaticRouter(BaseRouter):
    def get_route(self, log_entry: dict) -> str:
        # Security / critical paths to immutable store
        if _safelower(log_entry.get("LogSource")) == "openssh":
            return "ipfs"
        if _safelower(log_entry.get("Component")) == "kernel":
            return "ipfs"
        # Severity-based escalation
        lvl = _safelower(log_entry.get("Level"))
        if lvl in {"crit","alert","emerg"}:
            return "ipfs"
        if lvl in {"err","error","warn"}:
            return "elk"
        # Fallback: fast transactional tier
        return "mysql"
\end{lstlisting}

\paragraph{Q-Learning Inference Path.}
Inference uses (Scaler $\rightarrow$ PCA $\rightarrow$ Quantile Binning) to map continuous embeddings to a discrete key for tabular lookup; unseen states fall back to the teacher (static policy).
\begin{lstlisting}[language=Python,caption={Tabular Q-Learning router state discretization},label={lst:qrouter}]
class QLearningRouter(BaseRouter):
    def __init__(self, model_path_prefix="trained_models/q_learning"):
        # Load artifacts: Q-table, PCA, KBins, scaler, metadata
        self.q_table = pickle.load(open(f"{model_path_prefix}_q_table.pkl","rb"))
        self.pca = pickle.load(open(f"{model_path_prefix}_pca.pkl","rb"))
        self.binner = pickle.load(open(f"{model_path_prefix}_binner.pkl","rb"))
        self.scaler = pickle.load(open(f"{model_path_prefix}_scaler.pkl","rb"))
        self.metadata = json.load(open(f"{model_path_prefix}_metadata.json"))
        self.static = StaticRouter()
        # Compatibility guard
        if self.metadata.get("obs_dim") != self.scaler["mean"].shape[0]:
            self._invalidate("Obs dim mismatch")

    def _discretize_state(self, obs: np.ndarray) -> tuple | None:
        normed = (obs - self.scaler["mean"]) / self.scaler["std"]
        reduced = self.pca.transform(normed.reshape(1,-1))
        bins = self.binner.transform(reduced)
        return tuple(int(x) for x in bins[0])

    def get_route(self, log_entry: dict) -> str:
        system = get_system_state()                  # 6-dim system metrics
        emb = self.log_feature_extractor.get_embedding(log_entry.get("Content",""))
        obs = np.concatenate([system, emb]).astype(np.float32)
        key = self._discretize_state(obs)
        if key and key in self.q_table:
            a = int(np.argmax(self.q_table[key]))
            return {0:"mysql",1:"elk",2:"ipfs"}[a]
        return self.static.get_route(log_entry)
\end{lstlisting}

\paragraph{Q-Learning Training Loop.}
Guided exploration (teacher action during $\epsilon$-phase), prior bonus warm-start, adaptive epsilon decay on reward plateaus, and artifact emission (versioned metadata) were implemented.
\begin{lstlisting}[language=Python,
    caption={Adaptive Q-Learning training fragment},
    label={lst:qtrain},
    breaklines=true,
    breakatwhitespace=true
]
for ep in range(1, episodes+1):
    s = _disc_state(env.reset(), scaler, pca, binner)
    ep_reward = 0.0
    for _ in range(max_steps_per_episode):
        exploring = np.random.rand() < epsilon
        if exploring and np.random.rand() < guided_prob:
            teacher_act = {"mysql":0,"elk":1,"ipfs":2}[teacher.get_route(env.current_log)]
            a = teacher_act
        elif exploring:
            a = env.action_space.sample()
        else:
            a = int(np.argmax(Q[s]))
        obs2, r, done, _ = step(env, a)
        s2 = _disc_state(obs2, scaler, pca, binner)
        if s2 not in Q:  # warm-start toward teacher hint
            Q[s2] = np.zeros(n_actions, dtype=np.float32)
            t_act = {"mysql":0,"elk":1,"ipfs":2}[teacher.get_route(env.current_log)]
            Q[s2][t_act] = prior_bonus
        # Q-update
        td_target = r + (0 if done else gamma * np.max(Q[s2]))
        Q[s][a] += alpha * (td_target - Q[s][a])
        s = s2
        if not disable_static_eps_decay:
            epsilon = max(eps_end, epsilon * eps_decay)
        if done: break
    rewards_hist.append(ep_reward)
    # Plateau detection triggers multiplicative epsilon drop
\end{lstlisting}

\paragraph{A2C Training Enhancements.}
The learning rate linear decay and entropy coefficient annealing are modular callbacks; optional scaling of observations precedes the environment interaction.
\begin{lstlisting}[language=Python,
    caption={Adaptive Q-Learning training fragment},
    label={lst:qtrain},
    breaklines=true,
    breakatwhitespace=true
]
model = A2C("MlpPolicy", env,
            learning_rate=learning_rate,
            gamma=gamma, ent_coef=ent_coef, vf_coef=vf_coef,
            n_steps=n_steps, policy_kwargs={"net_arch":dict(pi=policy_hidden, vf=policy_hidden)},
            seed=seed, tensorboard_log=tensorboard_log)

callbacks = [EpisodeRewardCallback(csv_path=reward_csv_path)]
if lr_linear_decay:
    callbacks.append(LRScheduleCallback(total_timesteps=total_timesteps,
                                        initial_lr=learning_rate))
if ent_anneal and ent_target and ent_anneal_steps:
    callbacks.append(EntropyAnnealCallback(start_ent=ent_coef,
                                           target_ent=ent_target,
                                           anneal_steps=ent_anneal_steps))
if checkpoint_interval:
    callbacks.append(_CheckpointCallback(checkpoint_interval, save_base))

model.learn(total_timesteps=total_timesteps, callback=callbacks)
model.save(str(save_base))          # base.zip
# Metadata + optional scaler (_scaler.pkl) persisted afterwards
\end{lstlisting}

\paragraph{CBR Attribute Scoring \& Bucketed Statistics.}
Variance-reduction style scoring is recomputed periodically, and buckets are numeric hashes of attribute values.
\begin{lstlisting}[language=Python,caption={CBR attribute scoring loop},label={lst:cbr-score}]
def _score_attributes(self):
    global_vals = [x for v in self.global_stats.values() for x in v]
    gvar = variance(global_vals)
    scores = {}
    for attr, buckets in self.stats.items():
        wvar, total = 0.0, 0
        for b, backend_lat in buckets.items():
            merged = []
            for lst in backend_lat.values():
                merged.extend(lst)
            if len(merged) < 2: continue
            mean_b = mean(merged)
            var_b = variance(merged, mean_b)
            wvar += len(merged) * var_b
            total += len(merged)
        if total >= 5 and gvar > 1e-9:
            scores[attr] = max(0.0, gvar - (wvar / total)) / gvar
    if scores:
        self.attr_scores = scores
        self.classifier_attr = max(scores.items(), key=lambda p: p[1])[0]
        self._compute_expected()  # cache bucket->backend mean costs
\end{lstlisting}

\paragraph{Energy Measurement (RAPL + Optional GPU Integration).}
Energy attribution wraps the execution of backend operations; CPU package Joules are read from sysfs and (optionally) integrated with a rolling GPU sampler.
\begin{lstlisting}[language=Python,caption={Energy metering snapshot delta},label={lst:energy}]
def stop(self) -> EnergySample:
    now = self._read_snapshot()
    deltas = {k: max(0, now[k]-self._start_vals.get(k, now[k])) for k in now}
    pkg_uj = self._sum_pkg_uj(deltas)
    psys_uj = sum(d for k,d in deltas.items() if k.endswith("psys"))
    cpu_pkg_j = pkg_uj / 1e6
    psys_j = psys_uj / 1e6
    if psys_j > 0:
        gpu_est_j = max(0.0, psys_j - cpu_pkg_j)
    elif self._gpu_sampler:
        gpu_est_j = self._gpu_sampler.energy_between(self._t0, time.perf_counter())
    else:
        gpu_est_j = 0.0
    return EnergySample(duration_s=dt, cpu_pkg_j=cpu_pkg_j,
                        psys_j=psys_j, gpu_est_j=gpu_est_j)
\end{lstlisting}

\paragraph{Metadata Versioning.}
Artifacts include JSON metadata embedding structural, statistical, and compatibility-critical fields for forward validation (e.g., \texttt{obs\_dim}, PCA components, bin counts, reward statistics, and training schedules), enabling orchestration logic (Section~\ref{s:orchestration-unified-pipeline}) to detect drift.

\subsection{Artifact Directory Structure \& Naming Conventions}
This section enumerates the persisted artifacts and the rationale for each naming pattern, ensuring unambiguous reconstruction and automated reuse.

\paragraph{Top-Level Directories.}
\begin{description}
  \item[\texttt{trained\_models/}] Persisted learning artifacts (Q-learning tabular assets, A2C policies, optional scalers, metadata, checkpoints).
  \item[\texttt{results/}] Per-router per-log traces, per-router summaries, cross-router combined summaries, experiment metadata, optional compliance diagnostics.
  \item[\texttt{figures/}] Visualization outputs (latency distributions, throughput / latency trade-offs, energy breakdown).
  \item[\texttt{tables/}] Final processed tables (LaTeX/CSV) for publication.
  \item[\texttt{runs/}] (When TensorBoard logging enabled) scalar and histogram event data.
\end{description}

\paragraph{Q-Learning Artifact Set (prefix: \texttt{trained\_models/q\_learning}).}
\begin{center}
\begin{tabular}{ll}
\textbf{Suffix}\& \textbf{Content / Purpose} \\
\hline
\texttt{\_q\_table.pkl}\& Dict[state tuple $\rightarrow$ Q-vector] (float32) \\
\texttt{\_pca.pkl}\& Fitted \texttt{sklearn.decomposition.PCA} (dimensionality reduction) \\
\texttt{\_binner.pkl}\& \texttt{KBinsDiscretizer} (quantile bins over PCA output) \\
\texttt{\_scaler.pkl}\& Z-score scaler \{\texttt{mean}, \texttt{std}\} (float32 arrays) \\
\texttt{\_metadata.json}\& Versioned JSON: structural dims, hyperparams, reward stats, adaptive events \\
\end{tabular}
\end{center}
The shared prefix guarantees constant-time existence checks; the orchestrator compatibility logic reads \texttt{metadata.json} first to short-circuit loading if obsolete.

\paragraph{A2C Artifact Set (base name e.g.\ \texttt{trained\_models/a2c\_log\_router}).}
\begin{center}
\begin{tabular}{ll}
\textbf{Filename}\& \textbf{Purpose} \\
\hline
\texttt{a2c\_log\_router.zip}\& Final Stable-Baselines3 policy (weights + architecture) \\
\texttt{a2c\_log\_router\_best.zip}\& Best-eval snapshot (if evaluation + \texttt{--save\_best}) \\
\texttt{a2c\_log\_router\_metadata.json}\& Training provenance (RunMetadata schema v1) \\
\texttt{a2c\_log\_router\_scaler.pkl}\& Optional observation scaler (mean/std) \\
\texttt{a2c\_log\_router\_ckpt\_\emph{T}.zip}\& Periodic checkpoint at timestep \emph{T} \\
\texttt{a2c\_rewards.csv}\& (Optional) Per-episode reward curve (episode,reward) \\
\end{tabular}
\end{center}
Suffix conventions (\texttt{\_best}, \texttt{\_metadata}, \texttt{\_scaler}, \texttt{\_ckpt\_T}) enable glob-based grouping while avoiding conflicts with the primary policy basename.

\paragraph{Per-Router Evaluation Outputs.}
For the dataset name $X$ and router identifier $r$:
\begin{itemize}
  \item \texttt{results/}$r$\texttt{\_}$X$\texttt{.csv}: Per-log trace with latency components, energy, final destination, and (optionally) compliance override flags.
  \item \texttt{results/summary\_}$r$\texttt{\_}$X$\texttt{.csv}: Aggregated metrics (mean, median, p95 latency, success, energy, destination mix, compliance metrics if enabled).
\end{itemize}

\paragraph{Cross-Router Aggregation.}
\begin{itemize}
  \item \texttt{results/combined\_summary\_}$X$\texttt{.csv}: Unified table of all router summaries.
  \item \texttt{results/combined\_summary\_}$X$\texttt{.md}: (Optional) Markdown-formatted comparative table (for README inclusion).
  \item \texttt{results/experiment\_metadata\_}$X$\texttt{.json}: Orchestrator provenance (dataset fingerprint, which trainings reused vs retrained, timings, seed, compliance flags).
\end{itemize}

\paragraph{Compliance Diagnostics.}
When enabled:
\begin{itemize}
  \item Inline columns (\texttt{raw\_destination}, \texttt{compliance\_forced}) inside each per-log CSV.
  \item Aggregate compliance metrics embedded in per-router summary and combined summary (coverage, leakage, leakage rate, non-sensitive IPFS fraction, compliance score).
\end{itemize}

\paragraph{CBR State \& Diagnostics.}
\begin{itemize}
  \item \texttt{--cbr\_state\_path PATH}: If provided, JSON snapshot containing bucketed latency/energy statistics, selected attribute, decision counters.
  \item \texttt{--cbr\_json\_dump\_path PATH}: Periodic diagnostic dumps (single file overwrite, NDJSON append, or timestamped archive) containing attribute scores and current classifier.
\end{itemize}

\paragraph{Adaptive Q-Learning Supplemental Logs.}
\begin{itemize}
  \item \texttt{adaptive\_events.ndjson}: Stream of plateau-triggered epsilon drops (\{episode, epsilon\} per line).
  \item \texttt{reward\_history.csv}: Full reward trajectory externalized when not inlined (episodes exceed threshold).
\end{itemize}

\paragraph{Energy Acquisition Flags.}
A field within the experiment metadata notes RAPL availability; if unsupported, energy columns remain (zero or near-zero), but an explicit \texttt{"rapl\_available": false} (or analogous) flag prevents misinterpretation as true zero consumption.

\paragraph{Naming Rationale.}
The pattern \texttt{<base>[\_qualifier][.ext]} maintains:
\begin{enumerate}
  \item \textbf{Deterministic Discovery}: Orchestrator can test a fixed list of expected suffixes.
  \item \textbf{Collision Avoidance}: Each semantic layer (model, scaler, metadata, diagnostics) maps to a unique suffix.
  \item \textbf{Forward Compatibility}: Versioned metadata JSON mediates structural evolution without changing file basenames.
  \item \textbf{Human Parsability}: Filenames express both lineage (prefix) and function (suffix) without requiring directory nesting depth.
\end{enumerate}

\paragraph{Reconstruction Contract.}
Given:
\begin{itemize}
  \item Dataset fingerprint (hash + sampling mode)
  \item Artifact set (all \texttt{trained\_models/} files for chosen routers)
  \item Experiment metadata JSON
  \item (Optional) scaler and adaptive event logs
\end{itemize}
\noindent a third party can reproduce reported aggregate metrics by replaying the orchestrator with identical flags. Any structural drift (e.g., embedding dimensionality change) is surfaced by compatibility checks before silent degradation can occur.

\paragraph{Summary.}
The implementation layers (abstract router interface, modular artifact pipeline, adaptive learning callbacks, content-based online statistics, and energy attribution) are deliberately disentangled yet aligned through strict naming conventions and versioned metadata. This design minimizes hidden coupling, accelerates reproducible experimentation, and enables selective recomputation (train-once, evaluate-many) without the risk of stale or incompatible model reuse.

\section{Evaluation Methodology}
\label{s:evaluation-methodology}

\subsection{Core Metrics (Latency Distribution, Energy, Destination Mix)}
\label{s:eval-core-metrics}
We evaluate each router $r$ over an identical ordered log stream $\{\ell_i\}_{i=1}^{N}$ (dataset + fixed sampling mode), yielding per-log measurements:
\[
(t_i^{(r)},\; e_i^{(r)},\; d_i^{(r)},\; s_i^{(r)},\; z_i)
\]
where $t_i^{(r)}$ is end-to-end routing latency (ms) including backend service time, $e_i^{(r)}$ the CPU package energy (Joules) attributed to processing $\ell_i$ (Section~\ref{s:energy-measurement} reference if defined), $d_i^{(r)} \in \{\text{mysql},\text{elk},\text{ipfs}\}$ the selected destination (after any compliance override), $s_i^{(r)} \in \{0,1\}$ a success indicator (backend accept), and $z_i \in \{0,1\}$ a router-agnostic sensitive classification flag. All metrics are reported both per-router and---for comparative plots/tables---side-by-side across routers.

\paragraph{Latency Summary.}
We report the central tendency and tail sensitivity.
\[
\text{MeanLatency}(r)=\frac{1}{N}\sum_{i=1}^{N} t_i^{(r)}, \quad
\text{MedianLatency}(r)=\text{P}_{50}(\{t_i^{(r)}\}), \quad
\text{P95Latency}(r)=\text{P}_{95}(\{t_i^{(r)}\})
\]
\noindent The $(\text{Median}, \text{P95})$ pair captures distribution skew and tail amplification beyond mean effects. For the plots, we retained the full empirical CDF or kernel density (depending on the figure) to visualize the shape differences between the adaptive and baseline strategies.

\paragraph{Energy Metric.}
Average per-log energy:
\[
\text{EnergyMean}(r)=\frac{1}{N}\sum_{i=1}^N e_i^{(r)}.
\]
When hardware RAPL sampling is unavailable (all-zero traces), we still retain the column but mark an acquisition flag in the experiment metadata so that numerical zeros are not misconstrued as ideal efficiency. Where appropriate, we compute an energy-latency scalarized cost (weight $\lambda$ fixed ex ante) but emphasize disaggregated reporting to avoid masking divergent trade-offs.

\paragraph{Destination Mix.}
Let $\pi_b(r) = \frac{1}{N}\sum_{i=1}^N \mathbf{1}[d_i^{(r)}=b]$ for $b \in \{\text{mysql},\text{elk},\text{ipfs}\}$. Shifts in $\pi_{\text{ipfs}}$ quantify archival/integrity emphasis; changes in $\pi_{\text{elk}}$ capture analytical routing demand; $\pi_{\text{mysql}}$ reflects performance-tier load.

\paragraph{Success Reliability.}
\[
\text{SuccessRate}(r)=\frac{1}{N}\sum_{i=1}^N s_i^{(r)}.
\]
Although backend insertion failures are rare in practice, the metric bounds pathological policies that would trade correctness for cost.

\paragraph{Variance \& Confidence Intervals.}
For repeated runs (multiple seeds or dataset folds), we treat each run $k$ as yielding an estimate $\hat{\mu}_k$ (e.g., mean latency). The reported overall mean is $\bar{\mu} = \frac{1}{K}\sum_k \hat{\mu}_k$ with (when $K\ge 2$) a $95\%$ t-interval:
\[
\text{CI}_{95} = t_{0.975, K-1} \cdot \frac{\sigma_{\hat{\mu}}}{\sqrt{K}}, \quad 
\sigma_{\hat{\mu}}^2 = \frac{1}{K-1}\sum_k (\hat{\mu}_k - \bar{\mu})^2.
\]
We avoid overlapping CI visual clutter on percentile metrics (which are nonlinear functionals) and instead show violin or layered CDFs.

\paragraph{Tail Behavior Emphasis.}
Given operational SLO relevance, p95 is the primary tail statistic; p99 is optionally computed but not emphasized when sample size $N$ renders it too noisy (heuristic: hide p99 if $N < 5{,}000$ and router latencies show multimodal variance).

\paragraph{Streaming Implementation.}
Aggregation traverses per-log CSVs once, computing sums and storing latencies in an array (size $N$ manageable at the experimental scale). For larger deployments, the same formulas admit approximate quantile sketches (Greenwald--Khanna) without altering definitions.

\subsection{Compliance Metrics Interpretation}
\label{s:eval-compliance}
Compliance evaluation contextualizes governance, overhead, and protective coverage. Let sets:
\[
S = \{ i : z_i = 1\}, \qquad \bar{S} = \{ i : z_i = 0\}.
\]
For router $r$, with final (post-override) destination $d_i^{(r)}$:
\[
\text{SensitiveCoverage}(r) = \frac{\sum_{i \in S} \mathbf{1}[d_i^{(r)} = \text{ipfs}]}{|S|}, \quad
\text{Leakage}(r)= \sum_{i \in S} \mathbf{1}[d_i^{(r)} \ne \text{ipfs}],
\]
\[
\text{LeakageRate}(r)=\frac{\text{Leakage}(r)}{|S|}, \quad
\text{NonSensitiveIPFSFrac}(r)=\frac{\sum_{i \in \bar{S}} \mathbf{1}[d_i^{(r)}=\text{ipfs}]}{|\bar{S}|}, \quad
\text{ComplianceScore}(r)=\mathbf{1}[\text{Leakage}(r)=0].
\]

\paragraph{Interpretive Axes.}
\begin{enumerate}
  \item \textbf{Integrity Guarantee}: $\text{ComplianceScore}=1$ enforces zero leakage---a hard constraint, not an optimization objective.
  \item \textbf{Selectivity Cost}: Elevated \text{NonSensitiveIPFSFrac} indicates potential over-conservatism (unnecessary immutable storage usage).
  \item \textbf{Latency/Energy Overhead Attribution}: The difference in (MeanLatency, EnergyMean, $\pi_{\text{ipfs}}$) between paired baseline and compliance runs (Section~\ref{s:orchestration-compliance-pairing}) isolates governance impact.
  \item \textbf{Trade-off Envelope}: Plotting $\Delta \text{MeanLatency}$ vs.\ $\Delta \pi_{\text{ipfs}}$ locates routers on an overhead frontier—efficient policies dominate when they achieve high coverage (always 1 under hard enforcement) with minimal cost increase.
\end{enumerate}

\paragraph{Failure Modes.}
Any $\text{Leakage}(r)>0$ triggers experimental flagging; delta metrics for that router are discarded (fairness principle: only leak-free compliance comparisons are valid). A nonzero leakage implies pattern set insufficiency or regression in the override logic.

\paragraph{Governance vs Optimization Separation.}
Routers first produce a \ emph{raw}decision; compliance intercepts only afterward. Thus, learning objectives remain unbiased (no reward shaping distortion), ensuring that the reported compliance cost is an externally imposed constraint rather than an internalized behavior drift.

\subsection{Fairness Considerations (Frozen RL vs Adaptive CBR)}
\label{s:eval-fairness}
Ensuring an equitable comparison across heterogeneous routing paradigms (offline RL vs. online adaptive heuristics) requires explicit guardrails:

\paragraph{Frozen vs Online Learning.}
The Q-learning and A2C policies were frozen during the evaluation (weights, Q-table, PCA/binning, and scaler immutable). CBR is intrinsically online; its adaptation \emph{is} an inference mechanism. Freezing CBR would negate its design goal; conversely allowing RL to continue updating would introduce non-stationary evaluation bias. Therefore:
\[
\text{Train Phase (RL)} \;\;\Rightarrow\;\; \text{Freeze} \quad;\quad \text{Eval Phase (CBR)} \;\;\Rightarrow\;\; \text{Online Update}.
\]
This asymmetry is acknowledged and justified, and the reported metrics reflect the intended operational mode of each method.

\paragraph{Identical Input Stream.}
All routers consume the same ordered log sequence (identical dataset files, sampling strategy, and deterministic iteration). No router is advantaged by a different distribution slice; stochastic variation arises solely from internal decision making (none for frozen RL).

\paragraph{Uniform System State Context.}
System metrics collected at decision time (part of the observation vector) are measured once per log event so that the RL and Static/CBR paths leverage the same transient conditions (CPU load, memory indicators, etc.) without timing skew.

\paragraph{Instrumentation Neutrality.}
Latency timing includes router decision overhead \emph{and} backend persistence; therefore, methods with heavier feature transforms incur their genuine inference costs. Energy metering windows wrap the full routing + backend action; no method receives unmetered preprocessing.

\paragraph{Artifact Compatibility Enforcement.}
If structural changes (e.g., embedding dimensionality) would silently degrade an RL policy, orchestrator retraining is triggered (Section~\ref{s:orchestration-unified-pipeline}) rather than permitting suboptimal legacy artifacts, preventing biased underperformance.

\paragraph{Paired Compliance Deltas.}
The compliance impact is computed via paired evaluations differing only by the enforcement flag; the RL artifacts reused in both branches ensure delta purity (no retraining-induced variance).

\paragraph{Statistical Consistency.}
For any repeated-seed analyses (primarily RL training variability studies), we either (i) hold CBR fresh each time (reflecting realistic cold-start each deployment) or (ii) report clearly that a persistent CBR state file was reused (labelled) and never mixed states across runs without disclosure.

\paragraph{Limitations.}
Residual bias sources include: (a) potential advantage to CBR under temporal locality if the test stream orders clusters attribute values; (b) RL policies trained under one workload regime evaluated under slightly drifted conditions (distribution shift). Both are mitigated by dataset fingerprinting and (optionally) shuffling + fixed seed experiments (reported where applicable).

\paragraph{Summary.}
The fairness methodology codifies identical input distribution, frozen RL policy inference, honest instrumentation, compatibility revalidation, and paired compliance deltas. This scaffolding ensures that the observed performance differentials correspond to routing intelligence (or adaptation responsiveness) rather than experimental artifacts.
 
\section{Validation \& Reproducibility}\label{s:validation-reproducibility}
Ensuring that the reported performance, energy, and compliance outcomes are both \emph{valid} (they measure what is claimed) and \emph{reproducible} (they can be independently re-obtained under the stated conditions) is central to the credibility of this study. This section articulates the procedural and infrastructural controls that reduce threats to internal validity (measurement bias, configuration drift, and stochastic training variance) and external validity (hardware dependence and dataset sampling bias). We decompose our strategy into four pillars: (i) deterministic sampling and seeded pseudo‑randomness; (ii) containerized service orchestration with explicit capture of configuration and image digests; (iii) integrity safeguards for timing and energy instrumentation (latency wall‑clock isolation, RAPL domain verification, fallback flagging); and (iv) versioned metadata for all learned artifacts (state abstraction transformers, RL policies, adaptive router statistics) that enable compatibility checks and forensic traceability.

\subsection{Deterministic Sampling \& Seeds}
\label{subsec:deterministic_sampling}
To eliminate avoidable variance arising from stochastic data ordering and algorithmic randomness, we enforced a unified seeding and sampling protocol across all experimental runs. Let $S_{\text{global}}$ denote the user-specified master seed (or recorded default). From this single scalar, we derive disjoint, reproducible sub-seeds by hashing with context tags (e.g., \texttt{hash}(``q\_learning''$\Vert S_{\text{global}}$)), ensuring the independence of pseudo-random streams while preserving determinism. Concretely:
\begin{enumerate}
  \item \textbf{Dataset Ordering:} For modes requiring random selection (e.g., balanced or uniform down-sampling), we first load the full ordered log list, then apply a deterministic permutation $\pi$ generated by a PRNG initialized with $S_{\text{data}}$. Any head/prefix truncation or stratified extraction (e.g., balanced by severity) operates on $\pi(\text{logs})$, yielding a reproducible subsequence.
  \item \textbf{Reinforcement Learning (A2C / Q-Learning):} Three independent PRNGs (environment dynamics / observation noise, model parameter initialization, exploration policy) receive $S_{\text{env}}$, $S_{\text{model}}$, and $S_{\text{explore}}$ respectively, each derived from $S_{\text{global}}$. This separation prevents incidental coupling (e.g., identical parameter seeds altering exploration trajectories).
  \item \textbf{State Abstraction Transformers:} Collection of warm-up observations (for scaler mean/std, PCA fit, discretization bin edges) operates on the deterministically sampled log prefix; thus the learned transformation $\mathcal{T}$ is a pure function of $(S_{\text{global}}, \text{dataset version}, \text{feature extraction code hash})$.
  \item \textbf{Online CBR Adaptation:} Although CBR is inherently online, the decision stream it processes is the same deterministic log order used for all routers, ensuring that observed divergence is attributable to method behavior rather than input reordering.
  \item \textbf{Plateau / Adaptive Schedules:} Adaptive epsilon (Q-Learning) and entropy / LR schedules (A2C) consume only deterministic aggregated statistics (episode rewards at fixed evaluation intervals). Given identical seeds and code, schedule transition points (e.g., plateau-triggered epsilon drops) occurred at identical episodes.
\end{enumerate}
\paragraph{Recorded Provenance.} Each per-run metadata JSON persists: (i) $S_{\text{global}}$, (ii) derived sub-seeds, (iii) dataset filename plus a content hash (SHA-256) of the consumed CSV slice, (iv) counts of warm-up samples, and (v) feature extractor version (Git commit + embedding model identifier). This tuple suffices to reconstruct the exact input tensor sequence presented to each policy. If any incompatibility is detected at the load time (e.g., mismatch between the stored $\text{obs\_dim}$ and the current feature pipeline output), the system invalidates the artifact and triggers retraining (logged explicitly) rather than proceeding silently with misaligned state representations.
\subsection{Containerized Services \& Config Capture}
\label{subsec:containerization_config_capture}
To prevent configuration drift across experimental sessions and enable third parties to recreate the exact service environment, all stateful infrastructure components (MySQL, Elasticsearch/Logstash/Kibana (ELK), and the IPFS daemon) are launched via a declarative \texttt{docker-compose. yml}. This yields three reproducibility benefits: (i) immutable base images (tag + digest) pin library versions beneath the application layer, (ii) explicit, versioned environment variable settings and port mappings document externally visible service contracts, and (iii) isolated namespaces eliminate interference from host‐level transient processes. We adopted the following capture and verification pipeline:

\paragraph{Image Digest Freezing.} For each service image $I$ (for example, \texttt{mysql:8.0}, \texttt{elasticsearch:8. x}, \texttt{ipfs/go-ipfs:latest}), the resolved content digest $d(I)$ was recorded at the start of the experiment. The metadata file emits the tuple $(I, d(I))$; if a subsequent run resolves a different digest for the same tag, a warning is generated, and the new digest is appended, preserving an auditable chain.

\paragraph{Deterministic Service Startup.} Containers are created in a fixed order, and the orchestrator blocks until the health checks (e.g., MySQL TCP connect and Elasticsearch cluster green) succeed. A start barrier ensures that the initial latency measurements are not affected by delayed index creation or JIT warm-up. The elapsed stabilization time $t_{\text{stabilize}}$ was stored in the experimental metadata.

\paragraph{Configuration Snapshot.} Immediately after all services report healthy, the system exports:
\begin{itemize}
  \item MySQL: \texttt{SHOW VARIABLES} subset (buffer pool size, collation, isolation level).
  \item Elasticsearch: cluster settings and index mappings for log indices (queried via REST).
  \item IPFS: \texttt{ipfs config show} (filtered to non-secret keys).
\end{itemize}
Each snapshot is hashed (SHA-256) to produce $h_{\text{mysql}}, h_{\text{es}}, h_{\text{ipfs}}$; these hashes are embedded in the run metadata JSON. Full raw snapshots are optionally written to a \texttt{config\_snapshots/} directory for forensic comparison (disabled by default to keep the repositories lightweight).

\paragraph{Resource Constraints and Isolation.} The CPU (cpuset/quota) and memory limits declared in the compose file bound cross-run variability from host contention. When limits are absent (user override), a flag \texttt{resource\_constraints=false} is emitted, indicating that latency comparability may be weakened.

\paragraph{Clock and Timezone Normalization.} All containers inherit UTC to avoid time zone-induced timestamp shifts in the logs. The host monotonic clock is queried for latency instrumentation, and container wall times are used only for log timestamp echoing, making cross-container drift inconsequential.

\paragraph{Failure Transparency.} Any container restart event or non-zero exit during an experimental window is trapped and recorded with $(\text{service}, t_{\text{restart}}, \text{exit\_code})$. Runs with mid-experiment restarts were labeled \texttt{unstable=true} in the metadata, allowing exclusion in the aggregate statistical analyses.

\paragraph{Reconstruction Contract.} Given the artifacts: (i) \texttt{docker-compose. yml} (versioned in VCS), (ii) image digest list, (iii) configuration snapshot hashes, and (iv) global seed, an independent researcher can reconstruct an equivalent service layer. A mismatch in any hash during reconstruction constitutes provenance divergence and should be reported in replication studies.

This disciplined capture of container image identity and live configuration mitigates threats arising from silent upstream image updates, mutable runtime defaults, and opaque service reinitialization behavior.
\subsection{Integrity of Timing \& Energy Measurements}
\label{subsec:timing_energy_integrity}
Accurate latency and energy attribution underpin every comparative claim; any systemic bias propagates into cost and efficiency conclusions. Therefore, we isolate three potential threat surfaces—(i) clock source instability, (ii) contention/interference, and (iii) hardware counter misuse—and implement layered safeguards.

\paragraph{Monotonic Timing Source.} All per-log latencies are measured using a high-resolution monotonic clock (POSIX \texttt{CLOCK\_MONOTONIC\_RAW} or platform equivalent) rather than wall-clock time to eliminate effects from NTP adjustments or leap seconds. Each latency sample $\ell_i$ is computed as a single span (start/end) surrounding the full routing + backend write pipeline, thereby avoiding multi-fragment accumulation drift.

\paragraph{Synchronous Measurement Envelope.} The critical section for a log $i$—from pre-routing feature extraction through backend client acknowledgment—is bracketed; any asynchronous post-write tasks (e.g., JSON diagnostic dump, background scaler fitting) are executed outside this envelope to prevent artificial inflation of $\ell_i$. When unavoidable background work (e.g., periodic checkpoint writing) overlaps, the event is logged with its timestamp so that sensitivity analyses can exclude the affected windows.

\paragraph{Warm-Up Exclusion.} Just-in-time (JIT) effects (Python module imports, initial DB connection pooling, Elasticsearch JVM warm-up) predominantly affect the first $N_{warm}$ operations. We record a stabilization boundary $B$ (empirically determined by the convergence of a rolling median latency within 5\% over a 200-operation window) and flag early span samples ($i < B$). Summary statistics used for comparative plots exclude flagged warm-up unless explicitly noted; raw CSVs retain them for their transparency.

\paragraph{RAPL Domain Validation.} The energy measurements rely on the Intel RAPL package counters. At meter initialization, we enumerate the available domains; if the required package domain is absent or read attempts return non-increasing values, the run is annotated with \texttt{energy\_supported=false}. In such cases, the per-log energy defaults to 0.0 J, but a mandatory caveat column \texttt{energy\_valid=false} allows downstream aggregation to (a) ignore or (b) segregate zero-filled rows, preventing silent bias.

\paragraph{Per-Log Energy Attribution.} We adopt a differential snapshot method: for log $i$ we capture cumulative Joules $E_{pre}$ immediately before entering the critical section and $E_{post}$ immediately after. The incremental energy $\Delta E_i = \max(0, E_{post} - E_{pre})$ is attributed to the log. Negative or implausibly large outliers (beyond $\mu + 6\sigma$ over a sliding window) trigger a re-sampling attempt; persistent anomalies are recorded in an \texttt{energy\_anomalies} list within the metadata.

\paragraph{Clock / Counter Drift Checks.} Every $K$ operations (default $K=500$), we:
\begin{enumerate}
  \item Sample wall-clock and monotonic timestamps to ensure drift remains within a bounded tolerance (e.g., $<5$ ms difference in delta over the interval).
  \item Read the RAPL counter twice in rapid succession to verify monotonic advancement (difference $>0$). Failure increments a \texttt{rapl\_stall\_count}.
\end{enumerate}
A non-zero final \texttt{rapl\_stall\_count} is emitted, indicating potential under-at

\paragraph{Low-Noise Host Assumption.} No co-resident high-load processes were intentionally launched during the measurements. CPU frequency scaling governors are set to \texttt{performance} (when permitted), and this fact is recorded (\texttt{scaling\_governor=performance}); if the governor differs, we emit \texttt{governor\_warning=true}. This makes any increased variance risk explicit.

\paragraph{Energy + Latency Coupling Awareness.} Because energy attribution is differential, longer latency operations naturally accrue higher energy, even in the absence of additional work. For analysis, we compute Pearson correlation $\rho(\{\ell_i\}, \{\Delta E_i\})$; unusually high $\rho$ (e.g., $>0.95$ across all routers) could indicate either (i) genuine CPU-bound uniform workload, or (ii) insufficient measurement granularity. The correlation value was reported to contextualize the combined cost interpretations.

\paragraph{Reproducibility Contract.} Provided (a) raw per-log traces with flags (\texttt{warmup}, \texttt{energy\_valid}, anomalies), (b) monotonic vs wall-clock drift summary, and (c) metadata with RAPL support status and stall counts, an auditor can (i) recompute all aggregate statistics and (ii) detect any exclusion manipulations. No aggregate value in the manuscript relies on the unpublished transformation of these raw fields.

This multilayer instrumentation strategy constrains systematic errors, explicitly surfaces residual uncertainty, and preserves the ability to perform independent re-analysis without privileged access to ephemeral runtime states.
\subsection{Artifact Metadata \& Versioning}
\label{subsec:artifact_metadata_versioning}
Robust reproducibility requires that every learned or adaptive component be self-describing; an artifact must encode not only its parameters but also the provenance needed to validate structural compatibility and interpret performance. We codify this via explicit versioned metadata schemas per artifact class, forward-compatible loading with graceful invalidation, and cryptographic hashing of critical binary blobs.

\paragraph{Artifact Classes.} The system produces four principal artifact families.
\begin{enumerate}
  \item \textbf{Q-Learning Bundle} (version 3): \texttt{\_q\_table.pkl}, scaler (\texttt{\_scaler.pkl}), PCA transformer (\texttt{\_pca.pkl}), discretizer (\texttt{\_binner.pkl}), and \texttt{\_metadata.json}.
  \item \textbf{A2C Policy} (version 1): Stable-Baselines3 model (\texttt{.zip}), optional scaler (\texttt{\_scaler.pkl}), and \texttt{\_metadata.json}.
  \item \textbf{CBR State} (version 1, optional): Attribute bucket statistics snapshot (\texttt{cbr\_state.json}) plus inline header fields.
  \item \textbf{Experiment Aggregates} (version 1): Combined summary CSV / Markdown plus \texttt{experiment\_metadata\_<dataset>.json}.
\end{enumerate}

\paragraph{Schema Principles.} Each metadata JSON contains (i) a monotonically increasing integer \texttt{version}; (ii) structural descriptors (e.g., \texttt{obs\_dim}, \texttt{embedding\_dim}, PCA components, bin counts); (iii) training/runtime hyperparameters; (iv) statistical summaries (reward moments, epsilon/entropy schedule endpoints); (v) reproducibility anchors (global seed, git commit hash, dataset filename + content hash); and (vi) optional adaptive event logs.

\paragraph{Forward Compatibility \& Invalidation.} Under load, the router validates
\begin{enumerate}
  \item \emph{Dimensional Consistency:} Stored \texttt{obs\_dim} must match the current feature extractor output length. Mismatch $\Rightarrow$ artifact rejection.
  \item \emph{Embedding Subspace Stability:} If \texttt{embedding\_dim} differs (e.g., model upgrade 768$\rightarrow$1024), dependent transforms (scaler, PCA, bins) are deemed stale; automatic fallback to baseline routing is triggered and a retraining flag is emitted.
  \item \emph{Version Support:} Loader advertises a supported version range $[v_{\min}, v_{\max}]$. Artifacts with $v < v_{\min}$ raise a hard incompatibility (require migration script); artifacts with $v > v_{\max}$ raise a warning (attempted optimistic parse if backward compatible fields exist).
\end{enumerate}

\paragraph{Hashing \& Integrity.} For binary components (Q-table, PCA, scaler, discretizer, and A2C weights), we compute SHA-256 digests at creation time and store them in metadata under a \texttt{hashes} map keyed by logical name. On load, the digest is recomputed; mismatch sets \texttt{integrity\_verified=false} and aborts use (protecting against silent partial file corruption or accidental edits).

\paragraph{Adaptive Event Logging.} Q-learning adaptive epsilon drops and A2C entropy / learning rate schedule milestones are recorded as ordered event lists: each entry $\langle t, \text{type}, \text{value} \rangle$ where $t$ is episode or timestep. This allows the reconstruction of the dynamic exploration temperature without rerunning the training.

\paragraph{Minimal Sufficient Provenance Tuple.} For an RL policy, the tuple
\[
\left( \texttt{version}, \texttt{obs\_dim}, \texttt{embedding\_dim}, \texttt{policy\_arch\_hash}, \texttt{hyperparams\_hash}, \texttt{dataset\_hash}, \texttt{git\_commit}, \texttt{seed} \right)
\]
is mathematically sufficient to (a) detect structural mismatches, (b) map reward values to the correct scaling, and (c) differentiate this artifact from any other produced by the framework. We additionally include human-readable mirrors of architectural lists (e.g., hidden layer sizes) to support the manual inspection.

\paragraph{Migration Strategy.} Incrementing a schema version accompanies a documented CHANGELOG entry describing field additions/removals. A lightweight migration utility can up-convert $v \rightarrow v+1$ when the changes are additive (e.g., new optional statistics). Non-additive (breaking) changes require retraining, and the system explicitly refuses silent lossy conversion.

\paragraph{Graceful Degradation.} If an artifact fails any critical check, the runtime substitutes a deterministic fallback (static heuristic policy) instead of operating with an undefined behavior. A telemetry field \texttt{router\_degraded=true} is injected into per-log CSVs, enabling post-hoc filtering of degraded spans.

\paragraph{Reconstruction Guarantee.} Given the metadata JSON, the exact source revision (git commit), and the raw artifact binaries (whose digests match), an independent party can reinstantiate the identical inference pipeline (including scaler and PCA transforms) and verify the deterministic action selection for a fixed observation sequence. Divergence implies either (i) non-deterministic upstream dependencies (flagged) or (ii) unrecorded environmental variability, both of which are counted as reproducibility defects.

\paragraph{Retention Policy.} Intermediate (checkpoint) A2C models record their own hashes; the metadata notes \texttt{num\_checkpoints}. Old checkpoints may be pruned; their digests persist, so a later absence is distinguishable from a hash mismatch (preventing false integrity alarms).

This rigorous, versioned metadata architecture sharply limits silent failure modes (dimension drift, file corruption, undocumented hyperparameter changes) and supplies the cryptographic evidence necessary for third-party replication and forensic audits.
